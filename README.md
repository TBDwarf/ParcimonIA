# ParcimonIA ğŸ”€ğŸ’¸

> Router OpenWebUI intelligent pour optimiser les coÃ»ts d'IA en choisissant automatiquement le meilleur modÃ¨le LLM

[![License: Apache-2.0](https://img.shields.io/badge/License-Apache--2.0-blue.svg)](LICENSE)
[![Made in France](https://img.shields.io/badge/Made%20in-France-blue)](https://fr.wikipedia.org/wiki/France)

---

## ğŸ“‹ Table des matiÃ¨res

- [PrÃ©sentation](#-prÃ©sentation)
- [Comment Ã§a marche](#-comment-Ã§a-marche)
- [Cas d'usage](#-cas-dusage)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [RÃ©sumÃ© financier](#-rÃ©sumÃ©-financier)
- [Licence](#-licence)

---

## ğŸ¯ PrÃ©sentation

**ParcimonIA** est un pipe OpenWebUI qui optimise automatiquement vos dÃ©penses d'IA en sÃ©lectionnant le modÃ¨le le plus pertinent au meilleur coÃ»t. 

### Principe

Un nanoâ€‘modÃ¨le Ã©value la complexitÃ© de chaque requÃªte, puis le routeur `gpt-5-nano` choisit intelligemment entre `gpt-5-mini` (Ã©conomique) et `gpt-5` (performant) selon vos seuils, budget et politiques configurÃ©s.

### Objectif

**RÃ©duire votre facture d'IA** tout en maintenant la qualitÃ© perÃ§ue par l'utilisateur, avec observabilitÃ© complÃ¨te et mÃ©canismes de fallback.

### CompatibilitÃ©

> **Note :** Le code est actuellement conÃ§u pour Ãªtre utilisÃ© **nativement avec OpenAI** (GPT-5, GPT-5-mini, GPT-5-nano). Cependant, avec quelques modifications mineures du code (notamment au niveau des endpoints API et des paramÃ¨tres de requÃªte), ParcimonIA peut parfaitement fonctionner avec **d'autres fournisseurs de modÃ¨les** comme Anthropic Claude, Mistral AI, ou tout autre LLM compatible avec une API REST.

---

## âš™ï¸ Comment Ã§a marche

ParcimonIA intercale un "scorer" lÃ©ger avant chaque appel au modÃ¨le principal :

```
[Client] â†’ [Scorer ğŸ§®] â†’ [Router ğŸ”€] â†’ [gpt-5-mini | gpt-5] â†’ [RÃ©ponse]
```

### Workflow dÃ©taillÃ©

1. **RÃ©ception** de la requÃªte utilisateur
2. **Ã‰valuation** par `gpt-5-nano` qui produit un score de complexitÃ© (`light` vs `heavy`)
3. **Routage** selon vos rÃ¨gles (seuils, budget, fallbacks) :
   - `light` â†’ **gpt-5-mini** (Ã©conomique, rapide)
   - `heavy` â†’ **gpt-5** (raisonnement avancÃ©, tÃ¢ches complexes)
4. **Streaming** de la rÃ©ponse avec logs dÃ©taillÃ©s (modÃ¨le choisi, score, coÃ»t estimÃ©)
5. **ContinuitÃ©** : si un modÃ¨le a dÃ©jÃ  Ã©tÃ© fixÃ© dans l'Ã©change, le routeur le respecte automatiquement

---

## ğŸ’¡ Cas d'usage

### Cas 1 â€” Traduction simple (Ã©conomie automatique)

**Objectif** : Traduire un court paragraphe FRâ†’EN sans mise en forme complexe.

**Prompt utilisateur :**
```
Traduis le texte suivant en anglais, sans changer le sens ni le ton: 
"J'adore les promenades matinales au bord de la Seine."
```

**DÃ©cision :**
- `gpt-5-nano` classe la tÃ¢che en **light** (faible besoin de raisonnement)
- Le routeur sÃ©lectionne **gpt-5-mini**

**Pourquoi c'est pertinent :**
La traduction littÃ©rale de courts textes est un cas "pattern-based" oÃ¹ mini offre une qualitÃ© perÃ§ue proche de gpt-5, pour un coÃ»t bien infÃ©rieur.

**Log simplifiÃ© :**
```yaml
score_complexitÃ©: 0.18 â†’ classe: light
modÃ¨le: gpt-5-mini
tokens estimÃ©s: in=250, out=120
coÃ»t estimÃ©: 0,00X â‚¬
latence: faible
```

**Ã‰conomies :** ~90% par rapport Ã  gpt-5 sur ce type de requÃªte

---

### Cas 2 â€” GÃ©nÃ©ration de code (horloge universelle Python)

**Objectif** : GÃ©nÃ©rer un script robuste affichant l'heure UTC et locale, gÃ©rant fuseaux, formatage, arguments CLI et tests.

**Prompt utilisateur :**
```
Ã‰cris un script Python "universal_clock.py" qui:
- affiche l'heure UTC et l'heure locale
- accepte un argument --tz pour un fuseau IANA (ex: Europe/Paris)
- gÃ¨re les erreurs de fuseau
- inclut une fonction main et un petit test docstring
```

**DÃ©cision :**
- `gpt-5-nano` classe la tÃ¢che en **heavy** (besoin de raisonnement/code structurÃ©)
- Le routeur sÃ©lectionne **gpt-5**

**Pourquoi c'est pertinent :**
La gÃ©nÃ©ration de code correct avec gestion d'erreurs, interface CLI et tests nÃ©cessite plus de planification. GPT-5 couvre mieux les cas limites (TZ invalides, environnement sans tzdata, etc.) et produit une solution plus solide du premier coup, rÃ©duisant les itÃ©rations.

**Log simplifiÃ© :**
```yaml
score_complexitÃ©: 0.73 â†’ classe: heavy
modÃ¨le: gpt-5
tokens estimÃ©s: in=600, out=550
coÃ»t estimÃ©: 0,0Y â‚¬
latence: modÃ©rÃ©e
```

**Conclusion :** Pour le code, le raisonnement multiâ€‘Ã©tapes, le design d'API ou les intÃ©grations, gpt-5 rÃ©duit les allersâ€‘retours et livre un rÃ©sultat robuste.

---

## ğŸš€ Installation

### PrÃ©requis

- [OpenWebUI](https://openwebui.com/) installÃ© et configurÃ©
- ClÃ© API OpenAI valide

### Installation via l'interface OpenWebUI

1. **Ouvrez OpenWebUI**
   - AccÃ©dez au menu **Panneau d'administration**
   - Cliquez sur l'onglet **Fonctions**

2. **Ajoutez la fonction**
   - Cliquez sur **"Ajouter une fonction"**
   - Copiez/collez le contenu complet du fichier `.py` dans l'Ã©diteur
   - Donnez un nom Ã  la fonction : `ParcimonIA`

3. **Enregistrez**
   - Validez. Le pipe est crÃ©Ã© et activÃ©

4. **Configurez l'API**
   - Dans le menu **Vannes**, entrez votre clÃ© d'API OpenAI

5. **Utilisez le pipe**
   - Dans le sÃ©lecteur de modÃ¨le (zone de chat), sÃ©lectionnez **"ParcimonIA"**
   - Discutez normalement, le routage se fait automatiquement !

---

## ğŸ”§ Configuration

### Valves (paramÃ¨tres configurables)

| Valve | Description | Valeur recommandÃ©e |
|-------|-------------|-------------------|
| **OpenAI API Key** | Votre clÃ© d'API pour authentifier les appels | *Votre clÃ©* |
| **OpenAI API Base** | URL de base de l'API | `https://api.openai.com/v1` |
| **Light Model** | ModÃ¨le pour tÃ¢ches simples (traductions, reformulations, rÃ©sumÃ©s) | `gpt-5-mini` |
| **Heavy Model** | ModÃ¨le pour tÃ¢ches complexes (code, raisonnement, intÃ©grations) | `gpt-5` |
| **Routing Model** | ModÃ¨le analyseur qui Ã©value la complexitÃ© | `gpt-5-nano` |
| **Debug Routing** | Affiche la rÃ©ponse complÃ¨te du routeur (dev/optimisation) | `false` (prod) |
| **Show Model Used** | Affiche quel modÃ¨le a Ã©tÃ© choisi au dÃ©but de chaque rÃ©ponse | `true` |
| **Keep Model In Conversation** | Maintient le mÃªme modÃ¨le tout au long d'un Ã©change | `true` |

### RÃ©glages recommandÃ©s (prÃªts Ã  l'emploi)

```yaml
Light Model: gpt-5-mini
Heavy Model: gpt-5
Routing Model: gpt-5-nano
Debug Routing: false (en production)
Show Model Used: true
Keep Model In Conversation: true
```

### Astuces de forÃ§age manuel

Vous pouvez forcer un modÃ¨le spÃ©cifique dans votre requÃªte :

- **"Utilise ta capacitÃ© light."** â†’ Force `gpt-5-mini`
- **"Utilise ta capacitÃ© heavy."** â†’ Force `gpt-5`

---

## ğŸ’° RÃ©sumÃ© financier

### BÃ©nÃ©fice clÃ©

**Ã‰conomies sans friction** : le systÃ¨me dÃ©cide automatiquement quand utiliser gpt-5-mini (simple) ou gpt-5 (complexe).

### Ordre de grandeur

- Si `gpt-5-mini` est **5â€“15Ã— moins cher** que `gpt-5`
- Les tÃ¢ches simples routÃ©es vers mini gÃ©nÃ¨rent **80â€“93% d'Ã©conomies** par requÃªte
- Sur des centaines/milliers de requÃªtes simples par mois, l'Ã©cart devient **significatif**

### Exemple chiffrÃ©

Pour 370 tokens totaux (traduction simple) :
- **Avec gpt-5** : 0,20 â‚¬
- **Avec gpt-5-mini** : 0,02 â‚¬
- **Ã‰conomie** : ~0,18 â‚¬ soit **~90%**

> Les prix rÃ©els varient selon votre fournisseur. L'intÃ©rÃªt est de capter automatiquement ces Ã©conomies sans effort.

### Message Ã  retenir

Le gain principal est de **faire des Ã©conomies sans effort** sur les tÃ¢ches simples, tout en **sÃ©curisant la qualitÃ©** sur les tÃ¢ches exigeantes grÃ¢ce au routage automatique.

---

## ğŸ“„ Licence

**Copyright (c) 2025 TBDwarf - Tommy RENAULT**

Ce projet est distribuÃ© sous la **Apache License, Version 2.0**.  
Voir le fichier [LICENSE](LICENSE) Ã  la racine du dÃ©pÃ´t pour le texte complet.

> Merci de conserver cet avis dans vos distributions :  
> *"ParcimonIA Â© TBDwarf - Tommy RENAULT â€” Licence Apacheâ€‘2.0."*

---

## ğŸ­ Pourquoi "ParcimonIA" ?

Le nom **ParcimonIA** est un jeu de mots qui fusionne deux concepts clÃ©s du projet :

- **Parcimonie** : qualitÃ© de celui qui dÃ©pense avec mesure, Ã©conomie et sagesse.
- **IA** : Intelligence Artificielle, cÅ“ur technologique du projet.

**ParcimonIA** incarne ainsi une *IA parcimonieuse* : un systÃ¨me qui optimise intelligemment les ressources en ne mobilisant que la puissance de calcul strictement nÃ©cessaire pour chaque tÃ¢che.

---
## ğŸ™ Remerciements

Merci Ã  la communautÃ© OpenWebUI et Ã  tous les contributeurs qui rendent ce projet possible.

**Contributions bienvenues !**

---

<div align="center">

**Fait avec â¤ï¸ en France**

</div>
